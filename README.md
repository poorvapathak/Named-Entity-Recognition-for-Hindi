üß† Named Entity Recognition (NER) for Hindi using Hidden Markov Model (HMM)
üìö A Core NLP Approach for Indian Language NER

This project implements Named Entity Recognition (NER) for Hindi using a Hidden Markov Model (HMM) ‚Äî a classic probabilistic sequence labeling algorithm.
Instead of relying on deep neural networks like BERT or transformers, this work focuses on the core statistical and algorithmic foundation of NER.
It demonstrates how dynamic programming and linguistic feature engineering can be used to extract entities from Indian text in an interpretable, explainable way.

üè∑Ô∏è What is Named Entity Recognition (NER)?

Named Entity Recognition (NER) is a subtask of Natural Language Processing (NLP) that identifies and classifies real-world entities in text into predefined categories such as:

üë§ PER ‚Äì Person names (e.g., ‡§®‡§∞‡•á‡§Ç‡§¶‡•ç‡§∞ ‡§Æ‡•ã‡§¶‡•Ä)
üè¢ ORG ‚Äì Organizations (e.g., ‡§≠‡§æ‡§∞‡§§‡•Ä‡§Ø ‡§Ö‡§Ç‡§§‡§∞‡§ø‡§ï‡•ç‡§∑ ‡§Ö‡§®‡•Å‡§∏‡§Ç‡§ß‡§æ‡§® ‡§∏‡§Ç‡§ó‡§†‡§®)
üåç LOC ‚Äì Locations (e.g., ‡§¶‡§ø‡§≤‡•ç‡§≤‡•Ä, ‡§µ‡§æ‡§∞‡§æ‡§£‡§∏‡•Ä)
üìÖ DATE / EVENT ‚Äì Temporal or event expressions (e.g., ‡•®‡•¨ ‡§ú‡§®‡§µ‡§∞‡•Ä, ‡§ó‡§£‡§§‡§Ç‡§§‡•ç‡§∞ ‡§¶‡§ø‡§µ‡§∏)

NER is foundational for applications like information extraction, question answering, summarization, and knowledge graph building.

üîç Why Hindi and Indian Languages?
- NER in Indian languages poses unique challenges:
- No capitalization cues (unlike English)
- Rich morphology and inflections
- Code-mixing with English
- Multi-script variations and transliteration
- Sparse annotated corpora
Hence, this project explores how traditional probabilistic models (HMM) can be adapted for morphologically rich, low-resource languages like Hindi.

üß© Model Overview: Hidden Markov Model (HMM)
The Hidden Markov Model treats the text as a sequence of observed words generated by a sequence of hidden states (entity tags).

Components:
- Hidden States (Y): Named entity tags (PER, LOC, ORG, O)
- Observations (X): The words/tokens in the input sentence
- Initial Probabilities (œÄ): Probability of starting with each tag
- Transition Probabilities (A): Probability of one tag following another
- Emission Probabilities (B): Probability of observing a word given a tag

The model jointly estimates:

<img width="621" height="106" alt="image" src="https://github.com/user-attachments/assets/400b1092-ef3e-45b0-b669-e9df9bd09bd0" />

and uses Viterbi Algorithm for decoding ‚Äî a dynamic programming approach to find the most probable tag sequence for a given sentence.

üßÆ Workflow Overview
INPUT TEXT
    ‚Üì
TEXT TOKENIZATION
    ‚Üì
PART-OF-SPEECH TAGGING
    ‚Üì
LINGUISTIC FEATURE EXTRACTION
    ‚Üì
NAMED ENTITY RELATION MODEL (HMM)
    ‚Üì
SEQUENCE LABELING (Viterbi Algorithm)
    ‚Üì
CONTEXTUAL UNDERSTANDING
    ‚Üì
POST-PROCESSING
    ‚Üì
OUTPUT TAGGED ENTITIES

‚öôÔ∏è Implementation Details

üß∞ Environment:
- Language: Python
- Libraries: math, collections, re, random
- Platform: Google Colab / Jupyter Notebook

üóÇÔ∏è Dataset:
- Source: WikiANN Hindi
- Format: JSON lines with tokens and corresponding NER tags

Preprocessing:
- Removed non-Devanagari tokens
- Normalized tags (B-PER ‚Üí PER)
- Replaced rare words with <UNK>
Added suffix-based heuristic rules (e.g., -‡§™‡•Å‡§∞, -‡§®‡§ó‡§∞, -‡§µ‡§æ‡§≤‡§æ)

üí° Key Implementation Features
1. Data Cleaning & Normalization
Removes noise and ensures only valid Hindi tokens are retained:

def clean_token(token):
    token = re.sub(r'[^\u0900-\u097F]', '', token)
    return token.strip()

2. Laplace Smoothing
Ensures non-zero probability for unseen transitions and emissions:

P(word | tag) = (count(tag, word) + Œ±) / (count(tag) + Œ± * |V|)

3. Suffix-Based Hebbian Backoff
Heuristic probabilities based on morphological endings:

if word.endswith("‡§™‡•Å‡§∞") or word.endswith("‡§®‡§ó‡§∞"):
    prob = 0.8  # high chance of LOC

4. Viterbi Decoding
Dynamic programming to find best tag path:

delta[t][curr_tag] = max(delta[t-1][prev] + log(A[prev][curr_tag]) + log(B[curr_tag][word]))

5. Evaluation
Precision = TP / (TP + FP)
Recall = TP / (TP + FN)
F1 = 2 * (Precision * Recall) / (Precision + Recall)

üß† Example Output

Input Sentence:

‡§∂‡•ç‡§∞‡•Ä ‡§®‡§∞‡•á‡§Ç‡§¶‡•ç‡§∞ ‡§Æ‡•ã‡§¶‡•Ä ‡§¶‡§ø‡§≤‡•ç‡§≤‡•Ä ‡§ó‡§è‡•§

Predicted Tags:

[O, PER, PER, LOC, O, O]

Input Sentence:

‡§ü‡§æ‡§ü‡§æ ‡§ï‡§Ç‡§∏‡§≤‡•ç‡§ü‡•á‡§Ç‡§∏‡•Ä ‡§∏‡§∞‡•ç‡§µ‡§ø‡§∏‡•á‡§ú‡§º ‡§≠‡§æ‡§∞‡§§ ‡§ï‡•Ä ‡§∏‡§¨‡§∏‡•á ‡§¨‡§°‡§º‡•Ä ‡§ï‡§Ç‡§™‡§®‡•Ä ‡§π‡•à‡•§

Predicted Tags:

[ORG, ORG, ORG, LOC, O, O, O, O, O, O]

üìä Results
Metric	Score
Precision	0.318
Recall	0.301
F1-score	0.309

While these scores may seem modest, they are strong for:
- A pure HMM implementation (no ML library used)
- No deep model or pretrained embeddings
- Morphologically rich and noisy Hindi text

üîç Observations & Insights
- PER and LOC entities recognized reliably when preceded by honorifics or common suffixes.
- ORG entities occasionally misclassified due to overlapping tokens (e.g., ‡§≠‡§æ‡§∞‡§§ inside organization names).
- Suffix-based rules improved recall for LOC tags.
- Model fails on multi-word entities and code-mixed tokens ‚Äî a limitation of HMM‚Äôs single-word emission assumption.

üß© Challenges in Indian Language NER
- No capitalization features
- Agglutinative morphology
- Compound entity tokens
- Mixed script (Latin + Devanagari)
- Sparse training data and lack of standard corpora

üßæ What We Learnt
- HMM is a fundamental probabilistic sequence model that laid the foundation for CRFs and modern Transformers.
- Dynamic programming (Viterbi) is essential for global optimization in sequential tagging.
- Morphological and linguistic cues significantly improve classical model performance for Indian languages.
- Even ‚Äúold‚Äù models can perform meaningfully when engineered carefully with domain-specific heuristics.
- Interpretability and transparency make core NLP models valuable for research and education.

üìà Possible Future Work
- Integrate Bi-gram/Trigram HMMs for longer contextual dependency.
- Transition to Conditional Random Fields (CRF) for global normalization.
- Add gazetteer lists (city names, politicians, etc.) for semi-supervised learning.
- Explore Semi-Markov CRF to capture multi-token entities.
- Apply to other Indian languages (Marathi, Tamil, Bengali) for multilingual evaluation.

üßæ Example Visualization (from Notebook)
Sentence: ‡§µ‡§æ‡§∞‡§æ‡§£‡§∏‡•Ä ‡§â‡§§‡•ç‡§§‡§∞ ‡§™‡•ç‡§∞‡§¶‡•á‡§∂ ‡§Æ‡•á‡§Ç ‡§è‡§ï ‡§™‡•ç‡§∞‡§æ‡§ö‡•Ä‡§® ‡§∂‡§π‡§∞ ‡§π‡•à‡•§
Gold:     [LOC, LOC, LOC, O, O, O, O, O, O]
Pred:     [LOC, LOC, LOC, O, O, O, O, O, O]


‚úÖ Correct identification of compound location entity (LOC).
