ğŸ§  Named Entity Recognition (NER) for Hindi using Hidden Markov Model (HMM)
ğŸ“š A Core NLP Approach for Indian Language NER

This project implements Named Entity Recognition (NER) for Hindi using a Hidden Markov Model (HMM) â€” a classic probabilistic sequence labeling algorithm.
Instead of relying on deep neural networks like BERT or transformers, this work focuses on the core statistical and algorithmic foundation of NER.
It demonstrates how dynamic programming and linguistic feature engineering can be used to extract entities from Indian text in an interpretable, explainable way.

ğŸ·ï¸ What is Named Entity Recognition (NER)?

Named Entity Recognition (NER) is a subtask of Natural Language Processing (NLP) that identifies and classifies real-world entities in text into predefined categories such as:

ğŸ‘¤ PER â€“ Person names (e.g., à¤¨à¤°à¥‡à¤‚à¤¦à¥à¤° à¤®à¥‹à¤¦à¥€)

ğŸ¢ ORG â€“ Organizations (e.g., à¤­à¤¾à¤°à¤¤à¥€à¤¯ à¤…à¤‚à¤¤à¤°à¤¿à¤•à¥à¤· à¤…à¤¨à¥à¤¸à¤‚à¤§à¤¾à¤¨ à¤¸à¤‚à¤—à¤ à¤¨)

ğŸŒ LOC â€“ Locations (e.g., à¤¦à¤¿à¤²à¥à¤²à¥€, à¤µà¤¾à¤°à¤¾à¤£à¤¸à¥€)

ğŸ“… DATE / EVENT â€“ Temporal or event expressions (e.g., à¥¨à¥¬ à¤œà¤¨à¤µà¤°à¥€, à¤—à¤£à¤¤à¤‚à¤¤à¥à¤° à¤¦à¤¿à¤µà¤¸)

NER is foundational for applications like information extraction, question answering, summarization, and knowledge graph building.

ğŸ” Why Hindi and Indian Languages?
- NER in Indian languages poses unique challenges:
- No capitalization cues (unlike English)
- Rich morphology and inflections
- Code-mixing with English
- Multi-script variations and transliteration
- Sparse annotated corpora
Hence, this project explores how traditional probabilistic models (HMM) can be adapted for morphologically rich, low-resource languages like Hindi.

ğŸ§© Model Overview: Hidden Markov Model (HMM)

The Hidden Markov Model treats the text as a sequence of observed words generated by a sequence of hidden states (entity tags).

Components:

Hidden States (Y): Named entity tags (PER, LOC, ORG, O)

Observations (X): The words/tokens in the input sentence

Initial Probabilities (Ï€): Probability of starting with each tag

Transition Probabilities (A): Probability of one tag following another

Emission Probabilities (B): Probability of observing a word given a tag

The model jointly estimates:

ğ‘ƒ
(
ğ‘‹
,
ğ‘Œ
)
=
ğ‘ƒ
(
ğ‘Œ
1
)
âˆ
ğ‘¡
=
2
ğ‘‡
ğ‘ƒ
(
ğ‘Œ
ğ‘¡
âˆ£
ğ‘Œ
ğ‘¡
âˆ’
1
)
âˆ
ğ‘¡
=
1
ğ‘‡
ğ‘ƒ
(
ğ‘‹
ğ‘¡
âˆ£
ğ‘Œ
ğ‘¡
)
P(X,Y)=P(Y
1
	â€‹

)
t=2
âˆ
T
	â€‹

P(Y
t
	â€‹

âˆ£Y
tâˆ’1
	â€‹

)
t=1
âˆ
T
	â€‹

P(X
t
	â€‹

âˆ£Y
t
	â€‹

)

and uses Viterbi Algorithm for decoding â€” a dynamic programming approach to find the most probable tag sequence for a given sentence.

ğŸ§® Workflow Overview
INPUT TEXT
    â†“
TEXT TOKENIZATION
    â†“
PART-OF-SPEECH TAGGING
    â†“
LINGUISTIC FEATURE EXTRACTION
    â†“
NAMED ENTITY RELATION MODEL (HMM)
    â†“
SEQUENCE LABELING (Viterbi Algorithm)
    â†“
CONTEXTUAL UNDERSTANDING
    â†“
POST-PROCESSING
    â†“
OUTPUT TAGGED ENTITIES

âš™ï¸ Implementation Details
ğŸ§° Environment:

Language: Python

Libraries: math, collections, re, random

Platform: Google Colab / Jupyter Notebook

ğŸ—‚ï¸ Dataset:

Source: WikiANN Hindi

Format: JSON lines with tokens and corresponding NER tags

Preprocessing:

Removed non-Devanagari tokens

Normalized tags (B-PER â†’ PER)

Replaced rare words with <UNK>

Added suffix-based heuristic rules (e.g., -à¤ªà¥à¤°, -à¤¨à¤—à¤°, -à¤µà¤¾à¤²à¤¾)

ğŸ’¡ Key Implementation Features
1ï¸âƒ£ Data Cleaning & Normalization

Removes noise and ensures only valid Hindi tokens are retained:

def clean_token(token):
    token = re.sub(r'[^\u0900-\u097F]', '', token)
    return token.strip()

2ï¸âƒ£ Laplace Smoothing

Ensures non-zero probability for unseen transitions and emissions:

P(word | tag) = (count(tag, word) + Î±) / (count(tag) + Î± * |V|)

3ï¸âƒ£ Suffix-Based Hebbian Backoff

Heuristic probabilities based on morphological endings:

if word.endswith("à¤ªà¥à¤°") or word.endswith("à¤¨à¤—à¤°"):
    prob = 0.8  # high chance of LOC

4ï¸âƒ£ Viterbi Decoding

Dynamic programming to find best tag path:

delta[t][curr_tag] = max(delta[t-1][prev] + log(A[prev][curr_tag]) + log(B[curr_tag][word]))

5ï¸âƒ£ Evaluation
Precision = TP / (TP + FP)
Recall = TP / (TP + FN)
F1 = 2 * (Precision * Recall) / (Precision + Recall)

ğŸ§  Example Output

Input Sentence:

à¤¶à¥à¤°à¥€ à¤¨à¤°à¥‡à¤‚à¤¦à¥à¤° à¤®à¥‹à¤¦à¥€ à¤¦à¤¿à¤²à¥à¤²à¥€ à¤—à¤à¥¤

Predicted Tags:

[O, PER, PER, LOC, O, O]

Input Sentence:

à¤Ÿà¤¾à¤Ÿà¤¾ à¤•à¤‚à¤¸à¤²à¥à¤Ÿà¥‡à¤‚à¤¸à¥€ à¤¸à¤°à¥à¤µà¤¿à¤¸à¥‡à¤œà¤¼ à¤­à¤¾à¤°à¤¤ à¤•à¥€ à¤¸à¤¬à¤¸à¥‡ à¤¬à¤¡à¤¼à¥€ à¤•à¤‚à¤ªà¤¨à¥€ à¤¹à¥ˆà¥¤

Predicted Tags:

[ORG, ORG, ORG, LOC, O, O, O, O, O, O]

ğŸ“Š Results
Metric	Score
Precision	0.318
Recall	0.301
F1-score	0.309

While these scores may seem modest, they are strong for:

A pure HMM implementation (no ML library used)

No deep model or pretrained embeddings

Morphologically rich and noisy Hindi text

ğŸ” Observations & Insights

PER and LOC entities recognized reliably when preceded by honorifics or common suffixes.

ORG entities occasionally misclassified due to overlapping tokens (e.g., à¤­à¤¾à¤°à¤¤ inside organization names).

Suffix-based rules improved recall for LOC tags.

Model fails on multi-word entities and code-mixed tokens â€” a limitation of HMMâ€™s single-word emission assumption.

ğŸ§© Challenges in Indian Language NER

No capitalization features

Agglutinative morphology

Compound entity tokens

Mixed script (Latin + Devanagari)

Sparse training data and lack of standard corpora

ğŸ§¾ What We Learnt

HMM is a fundamental probabilistic sequence model that laid the foundation for CRFs and modern Transformers.

Dynamic programming (Viterbi) is essential for global optimization in sequential tagging.

Morphological and linguistic cues significantly improve classical model performance for Indian languages.

Even â€œoldâ€ models can perform meaningfully when engineered carefully with domain-specific heuristics.

Interpretability and transparency make core NLP models valuable for research and education.

ğŸ“ˆ Possible Future Work

Integrate Bi-gram/Trigram HMMs for longer contextual dependency.

Transition to Conditional Random Fields (CRF) for global normalization.

Add gazetteer lists (city names, politicians, etc.) for semi-supervised learning.

Explore Semi-Markov CRF to capture multi-token entities.

Apply to other Indian languages (Marathi, Tamil, Bengali) for multilingual evaluation.

ğŸ§® Project Structure
ğŸ“¦ NLP_Mini_Project
 â”£ ğŸ“œ NLP_Mini-Project.ipynb
 â”£ ğŸ“Š Dataset (WikiANN Hindi)
 â”£ ğŸ“˜ NLP_NER_39_41_45.pdf        â† Project Presentation
 â”£ ğŸ§¾ README.md                   â† This file
 â”— ğŸ“ˆ Results and Outputs

ğŸš€ How to Run
1ï¸âƒ£ Clone Repository
git clone https://github.com/<your-username>/hindi-ner-hmm.git
cd hindi-ner-hmm

2ï¸âƒ£ Open Notebook
jupyter notebook NLP_Mini-Project.ipynb

3ï¸âƒ£ Run All Cells

The notebook will:

Load WikiANN data

Train HMM model from scratch

Decode sequences using Viterbi

Print predictions and evaluation metrics

ğŸ§¾ Example Visualization (from Notebook)
Sentence: à¤µà¤¾à¤°à¤¾à¤£à¤¸à¥€ à¤‰à¤¤à¥à¤¤à¤° à¤ªà¥à¤°à¤¦à¥‡à¤¶ à¤®à¥‡à¤‚ à¤à¤• à¤ªà¥à¤°à¤¾à¤šà¥€à¤¨ à¤¶à¤¹à¤° à¤¹à¥ˆà¥¤
Gold:     [LOC, LOC, LOC, O, O, O, O, O, O]
Pred:     [LOC, LOC, LOC, O, O, O, O, O, O]


âœ… Correct identification of compound location entity (LOC).

âœï¸ Authors

Poorva Pathak
Department of Artificial Intelligence and Data Science
Vivekanand Education Societyâ€™s Institute of Technology (VESIT), Mumbai
